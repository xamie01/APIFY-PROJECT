"""
Unified wrapper for interacting with different AI providers
"""

import os
import time
from typing import Dict, Any, Optional, List
from abc import ABC, abstractmethod
import openai
from anthropic import Anthropic
import google.generativeai as genai
from .logger import get_logger
from .utils import get_api_key, load_config

logger = get_logger(__name__)


class BaseAIProvider(ABC):
    """Abstract base class for AI providers"""
    
    @abstractmethod
    def query(self, prompt: str, **kwargs) -> str:
        """Send a query to the AI and return response"""
        pass
    
    @abstractmethod
    async def query_async(self, prompt: str, **kwargs) -> str:
        """Send an async query to the AI and return response"""
        pass


class OpenAIProvider(BaseAIProvider):
    """OpenAI API provider"""
    
    def __init__(self, model: str = "gpt-4", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or get_api_key("openai")
        
        if not self.api_key:
            raise ValueError("OpenAI API key not found")
        
        self.client = openai.OpenAI(api_key=self.api_key)
        logger.info(f"OpenAI provider initialized with model: {model}")
    
    def query(self, prompt: str, **kwargs) -> str:
        """
        Send a query to OpenAI
        
        Args:
            prompt: User prompt
            **kwargs: Additional parameters
        
        Returns:
            AI response text
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 2000),
            )
            
            return response.choices[0].message.content
            
        except openai.OpenAIError as e:
            logger.error(f"OpenAI API error: {e}")
            raise
    
    async def query_async(self, prompt: str, **kwargs) -> str:
        """
        Send an async query to OpenAI
        
        Args:
            prompt: User prompt
            **kwargs: Additional parameters
        
        Returns:
            AI response text
        """
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 2000),
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise


class AnthropicProvider(BaseAIProvider):
    """Anthropic Claude API provider"""
    
    def __init__(self, model: str = "claude-3-opus-20240229", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or get_api_key("anthropic")
        
        if not self.api_key:
            raise ValueError("Anthropic API key not found")
        
        self.client = Anthropic(api_key=self.api_key)
        logger.info(f"Anthropic provider initialized with model: {model}")
    
    def query(self, prompt: str, **kwargs) -> str:
        """
        Send a query to Anthropic Claude
        
        Args:
            prompt: User prompt
            **kwargs: Additional parameters
        
        Returns:
            AI response text
        """
        try:
            message = self.client.messages.create(
                model=self.model,
                max_tokens=kwargs.get("max_tokens", 2000),
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            return message.content[0].text
            
        except Exception as e:
            logger.error(f"Anthropic API error: {e}")
            raise
    
    async def query_async(self, prompt: str, **kwargs) -> str:
        """Async version of query"""
        return self.query(prompt, **kwargs)


class GeminiProvider(BaseAIProvider):
    """Google Gemini API provider"""
    
    def __init__(self, model: str = "gemini-pro", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or get_api_key("google") or os.getenv("GOOGLE_API_KEY")
        
        if not self.api_key:
            raise ValueError("Google API key not found. Please set GOOGLE_API_KEY in your .env file")
        
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(model)
        logger.info(f"Gemini provider initialized with model: {model}")
    
    def query(self, prompt: str, **kwargs) -> str:
        """
        Send a query to Google Gemini
        
        Args:
            prompt: User prompt
            **kwargs: Additional parameters
        
        Returns:
            AI response text
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            raise
    
    async def query_async(self, prompt: str, **kwargs) -> str:
        """
        Send an async query to Google Gemini
        """
        try:
            response = await self.model.generate_content_async(prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"Gemini API error: {e}")
            raise


class TargetAIWrapper:
    """
    Main wrapper class that provides a unified interface to all AI providers
    """
    
    def __init__(self, target: str, config: Optional[Dict[str, Any]] = None):
        """
        Initialize AI wrapper
        
        Args:
            target: Target AI identifier (e.g., 'openai-gpt4', 'anthropic-claude')
            config: Optional configuration dictionary
        """
        self.config = config or load_config()
        self.target = target
        self.provider = self._initialize_provider(target)
        
        # Request tracking
        self.request_count = 0
        self.total_tokens = 0
        self.request_history: List[Dict[str, Any]] = []
        
        logger.info(f"Target AI wrapper initialized: {target}")
    
    def _initialize_provider(self, target: str) -> BaseAIProvider:
        """
        Initialize the appropriate provider based on target
        
        Args:
            target: Target AI identifier
        
        Returns:
            Initialized provider instance
        """
        target_lower = target.lower()
        
        if target_lower.startswith("openai"):
            if "-" in target:
                model = target.split("-", 1)[1]
                # Fix common model name issues
                if model == "gpt3.5":
                    model = "gpt-3.5-turbo"
                elif model == "gpt4":
                    model = "gpt-4"
                elif model.startswith("gpt"):
                    # Remove duplicate 'gpt-' prefix if present
                    model = f"gpt-{model.replace('gpt', '')}"
                elif not model.startswith("gpt-"):
                    model = f"gpt-{model}"
            else:
                model = "gpt-4"
            return OpenAIProvider(model=model)
        
        elif target_lower.startswith("gemini"):
            # Always use gemini-pro unless explicitly requesting vision model
            if target_lower.endswith("vision"):
                model = "gemini-pro-vision"
            else:
                model = "gemini-pro"
            return GeminiProvider(model=model)
        
        elif target_lower.startswith("anthropic"):
            model = target.split("-", 1)[1] if "-" in target else "claude-3-opus-20240229"
            return AnthropicProvider(model=model)
        
        else:
            raise ValueError(f"Unsupported target AI: {target}")
    
    def query(self, prompt: str, **kwargs) -> str:
        """
        Send a query to the target AI
        
        Args:
            prompt: User prompt
            **kwargs: Additional parameters
        
        Returns:
            AI response text
        """
        start_time = time.time()
        
        try:
            self._apply_rate_limit()
            response = self.provider.query(prompt, **kwargs)
            
            execution_time = time.time() - start_time
            self._track_request(prompt, response, execution_time)
            
            logger.debug(f"Query completed in {execution_time:.2f}s")
            return response
            
        except Exception as e:
            logger.error(f"Query failed: {e}")
            raise
    
    async def query_async(self, prompt: str, **kwargs) -> str:
        """
        Send an async query to the target AI
        
        Args:
            prompt: User prompt
            **kwargs: Additional parameters
        
        Returns:
            AI response text
        """
        start_time = time.time()
        
        try:
            self._apply_rate_limit()
            response = await self.provider.query_async(prompt, **kwargs)
            
            execution_time = time.time() - start_time
            self._track_request(prompt, response, execution_time)
            
            logger.debug(f"Async query completed in {execution_time:.2f}s")
            return response
            
        except Exception as e:
            logger.error(f"Async query failed: {e}")
            raise
    
    def _apply_rate_limit(self) -> None:
        """Apply rate limiting based on configuration"""
        max_rpm = self.config.get("target_ai", {}).get("rate_limit_requests_per_minute", 30)
        
        if len(self.request_history) >= max_rpm:
            oldest_request = self.request_history[0]
            time_since_oldest = time.time() - oldest_request["timestamp"]
            
            if time_since_oldest < 60:
                sleep_time = 60 - time_since_oldest
                logger.warning(f"Rate limit reached, sleeping for {sleep_time:.2f}s")
                time.sleep(sleep_time)
            
            cutoff_time = time.time() - 60
            self.request_history = [
                r for r in self.request_history 
                if r["timestamp"] > cutoff_time
            ]
    
    def _track_request(self, prompt: str, response: str, execution_time: float) -> None:
        """Track request metrics"""
        self.request_count += 1
        
        prompt_tokens = len(prompt.split()) * 1.3
        response_tokens = len(response.split()) * 1.3
        total_tokens = prompt_tokens + response_tokens
        
        self.total_tokens += total_tokens
        
        request_data = {
            "timestamp": time.time(),
            "prompt_length": len(prompt),
            "response_length": len(response),
            "estimated_tokens": total_tokens,
            "execution_time": execution_time
        }
        
        self.request_history.append(request_data)
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get usage metrics"""
        return {
            "total_requests": self.request_count,
            "total_tokens": self.total_tokens,
            "average_response_time": sum(r["execution_time"] for r in self.request_history) / 
                                    len(self.request_history) if self.request_history else 0,
            "requests_last_minute": len([
                r for r in self.request_history 
                if time.time() - r["timestamp"] < 60
            ])
        }
    
    def reset_metrics(self) -> None:
        """Reset all metrics"""
        self.request_count = 0
        self.total_tokens = 0
        self.request_history = []
        logger.info("Metrics reset")
